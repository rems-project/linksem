We thank the reviewers for their helpful comments.  We first note that the
reviewers collectively ask for three things:

  1. Using the model to prove interesting theorems about the linking
  process (A, D); 
  2. Using "lessons learnt" from the specification process to
  investigate how linking could be made less ad-hoc (B, D, E); and
  3. Extending existing verified compilers such as CompCert to handle
  linking (C).

Each of these would be another (very interesting, multi-person-year)
project in its own right, and trying to combine any of them with the
current work would take it beyond what can reasonably be explained in
a single PLDI paper.  The fact that one can start seriously
contemplating such work is itself a contribution of our current paper,
and this is exactly what we meant when we wrote of "bringing linking
into the formal discourse".


To address some other important points:

  * For E: our formal model is included as supplementary material, and
    we intend to make it available under an open-source licence before
    publication.

  * Reviewers B and D expected to see the "essence" of linking.  While
    (we agree) it would be great if such a thing did exist, a clear
    conclusion from our work is that it does not; instead, there are
    many important use-cases for different aspects of linker-speak,
    which are supported by many more-or-less ad hoc mechanisms.  The
    behaviour of current linkers is essentially a contract between
    hundreds of disparate compilers and other tools, that now rely on
    this behaviour in order to work correctly, so it cannot simply be
    "fixed" in isolation.  Instead, we have to understand all those
    use-cases and mechanisms in depth before going forwards.

    D writes "At the end of the paper, I had basically learned that
    linking is an arcane activity perhaps in need of a re-think. I did
    not have any larger insight than I did when I started."  As is
    often the case, the devil is in the details.  If one wants to
    pursue this ((2) above), our work provides a concise and precise
    characterisation of many of the details that will have to be
    addressed.

  * For A: *all* recursive functions, both in the ELF and linker
    formalisation, have been proved terminating.  This already
    establishes the basic usability of the model for proof.

  * For A: while "hello world" is close to the simplest C
    program, linking it exercises many aspects of linking. 

  * Reviewers A and D suggest that the sole point of a formalisation
    is to support proof, and question the relationship between a model
    and an implementation in a restrictive language.  What we have is
    a "model" in several important ways: (a) it is mathematically
    precise, (b) it is written with clarity as the prime goal, and (c)
    it explicitly addresses the various forms of loose specification
    involved here rather than fixing on some implementation choice.
    Supporting proof is only one possible goal among many of a
    formalisation: we are equally concerned with providing a clear
    specification, supporting discussion at a level of abstraction
    above C implementations, and providing a test oracle.

 


---------------- BITS --------------------

Proof is only one point of formalisation.  Another is communication at a
    level of abstraction above C implementations.  Another is enabling of further
    research (also not necessarily formal proof), as mentioned at the start of
    this reply.



    Further, we wish to stress that our avoiding glibc is not related to non-
    determinism, but rather problems with handling of IRELATIVE.  Further, our
    linker has been validated against both Gold and BFD, not just BFD alone.



    "we already knew that linking was messy" may be true for some expert
    reviewers, but looking at previous formal treatments of linking,
    where the focus has been on modularisation and separate
    compilation to the exclusion of anything else, it's clear that
    there's a sizable group of people who are *not* aware of the full
    generality or complexity of the linking process. Claims that we
    but are certainly not universally true within the PL, compiler,
    and wider theory communities.



    (At the time of submission one very large recursive function was
    problematic due to a bug we reported on the Isabelle mailing list;
    it has since been rewritten to avoid that Isabelle problem.)

    Since submission all functions involved in the linker are now proved
    terminating.  At the time of submission one very large recursive function
    was causing problems (assign_inputs_to_outputs in linker_script.lem) due
    to a bug in Isabelle that we reported to the developers.  Since submission this
    function has been rewritten by hand to avoid the exception that Isabelle was
    throwing and proved terminating.  

------------------------------------------


> 
> Review #3A		X	C
> Review #3B		Y	B
> Review #3C		X	B
> Review #3D		X	D
> Review #3E		X	A
> 
> ===========================================================================
>                             PLDI '16 Review #3A
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: C. Weak paper, though I will not fight
>                                         strongly against it
> 
>                          ===== Paper summary =====
> 
> This paper argues that complete and correct compilation and program
> analysis requires a more formal representation and understanding of
> linking. In support of this goal, it reimplements a large subset of an
> ELF linker in the style of GNU ld, in a way that goes easily into the
> Isabelle/HOL proof assistant. As motivation, the paper gives examples
> of several subtleties of ELF linking, including some features which
> change or break the otherwise-standard semantics of C. The paper's
> specification of the ELF file format is verified by I/O comparison
> experiments versus GNU readelf over a large corpus of binaries. The
> paper documents a variety of sources of non-determinism in linking,
> and describes an approach of supplementing a non-deterministic
> specification with a "personality function" to emulate the choices of
> a particular existing implementation, but the personality function for
> GNU ld is not yet accurate enough to produce matching outputs for
> realistic-sized examples. The model linker is however able to produce
> a working hello-world binary when linking with uClibc. As first steps
> in using the model for formal reasoning, the author(s) have supplied
> termination proofs to allow the ELF file format model to be used in
> Isabelle/HOL, and stated in Isabelle/HOL but not proved a correctness
> property of x64 relocation in terms of the instruction semantics. The
> paper ends by laying out a variety of directions for future working
> building on a precise understanding of linkers, including improving
> specifications, program analysis, verified compilation, and
> improvements to the linking interface.
> 
>                       ===== Comments for author =====
> 
> Key strengths and weaknesses:
> 
> + The paper provides a good overview of linker features
> 
> + The paper demonstrates that linking is important to program
> semantics, but complex and poorly specified
> 
> + The specification approach seems clean, supporting both execution
> and use in proofs
> 
> - The model linker seems to yet scale only to small programs and a C
> library that, while large, is not the standard Linux one

(Stephen can perhaps provide anecdotes of doing better)

> - No interesting theorems have yet been proved

Whilst this is true (modulo our comment below about proving termination of functions
used to define the linker itself) we believe that proving anything interesting about
the model is at *least* another paper's worth of work, and there's a limit to what
we can sensibly report (and do) within the confines of a single paper.

> I found this paper fun and easy to read, and it serves well as a
> survey of surprisingly-complex linker mechanisms and as a position
> paper on why a more formal understanding of them would be worthwhile.
> However the actual formalization work that I would have expected to be
> the core of the paper feels preliminary and is not so impressive.
> Modeling a linker without proving anything about it is basically just
> reimplementing a linker using a restrictive programming language.  The
> level of detail the paper gives about how it does this makes it sound
> well-structured and suitable as a basis for future work, but a real
> demonstration of its value would be by constructing proofs. The
> theorem about relocation correctness that the paper describes
> formulating would sounds like a good demonstration, but if the
> author(s) haven't yet proved it, we don't have much confidence that
> its statement is correct. This feels like a worthwhile project, so I'm
> hoping that what happened is just that the PLDI submission deadline
> came too soon. It sounds like with a few more compatibility changes
> and bugfixes the model linker could be a robust tool, and if the
> author(s) can prove some basic facts about it I think the value of the
> formalization would be more clear.

repeat: lots more work to make this happen

> I'd missed the distinction on a first reading, but on looking back,
> the section structure seems to suggest that the termination proofs
> apply only to the model of the ELF file format, and not to the model
> linker. On the other hand the mention in the introduction does not
> make this distinction, so I guess I'd say the paper is ambiguous. By
> analogy, are the 1500 lines of handwritten termination proofs are
> roughly proving the termination of ELF file parsing as in "readelf",
> or the termination of linking as in "ld"? The latter would be a
> somewhat more interesting result.

They are both, and we will rewrite this text to make this clear.  We prove both
the termination of functions related to the "readelf"-like tool and to functions
related to our linker.  Note, that a lot of the complex functions that actually
do the linking process (in link.lem and Link.thy, and similar) are not recursive and
therefore are terminating by fiat.

We should point out, however, that in Linker_script.thy submitted with this paper
one large recursive function is commented out in the Isabelle/HOL extraction, along
with several other non-recursive functions that depend upon this.  For some reason,
this definition causes Isabelle to issue an exception from somewhere deep within
its definition package: a bug.  This was reported to the Isabelle mailing list in
late 2015 and we are still waiting on a fix.  In the meantime, after submission,
this function was rewritten by hand into a form that Isabelle will accept, and
proved terminating, along with all functions that depend upon it.  As a result,
all recursive functions, whether part of the ELF model or the linker itself, are
now terminating in our local copy of the model.

> The clearest example I understood from the paper about the behavior of
> the linker breaking what should be C language guarantees is the
> example of references to weak symbols being null. The case for the
> practical importance of the paper's work would be stronger, though, if
> the paper could give a more pernicious example of linking causing
> undesirable behavior, such as something silently incorrect, or clearly
> contrary to programmer intent. Perhaps this position could be filled
> by expanding on the claim in the introduction about Figure 1 that the
> calls to _int_malloc might not go the function defined in that figure?
> It wasn't clear to me which linker behavior could be the cause of
> that.

can point at the bit that refers back to it (substitution)

> I would have been curious to hear a bit more about the further bugs in
> the ELF specification that were revealed not by testing on 7k binaries
> but by the Isabelle/HOL proof process for termination and "various
> other lemmas".

First, I should clarify that Isabelle proofs and validation have been carried
out concurrently, and it is not the case that work in Isabelle started only after
validation was complete.  As a result, several very basic errors were in fact
highlighted not by validation, but by preliminary work in Isabelle where we
experimented with proof using the model, including an incorrect byte
ordering in the parsing function for 8 word wide types like elf64_xword that caused
roundtripping of parsing and then blitting of values of this type to fail.  This
would eventually have been identified by validation, especially our validation
of the rountripping property using hexdump, but was in fact identified within
Isabelle itself.

Some bugs that would not have been identified by validation include certain
functions that did not terminate on all inputs, in the linker and in the ELF
model, for example parsing functions relating to symbol versioning were originally
not provably terminating on all inputs due to a silly error and this was only
brought to light when we attempted to prove them so.  Bugs of this sort
would only have been uncovered by validation if extremely lucky
(and having a failing termination proof makes tracking down the cause of non-
termination much easier than just observing a non-terminating program).

Another recent bug in the specification uncovered by working with the model in
Isabelle was a lack of padding in NOTE sections to align certain fields up to a
64- or 32-bit boundary.  This was not uncovered during validation as the actual
contents of NOTE sections were never systematically decoded during validation
(readelf does not do this for --program-headers and --section-headers).

Note also that bugs in ABI handling were uncovered by us using our model with
external tools (i.e. ppcmem2).  Correct interpretation of the program entry point
was only identified by trying to use our ELF code to read in ARM and PowerPC
binaries and noting incorrect calculation of the program entry point on ARM vs.
PowerPC.

In short: Isabelle extraction was used relatively early and new versions of the
model have continuously been extracted to Isabelle, proofs and definitions updated,
and so on, and has played a role in validation of even basic definitions itself,
as has trying to use our ELF model with other tools as a library.

> I also feel like I should comment a bit on anonymization. The
> author(s) removed identifying information in several places, but I
> think they hurt likely their degree of anonymity by mentioning that
> the model described in this paper has already been used in the ppcmem2
> system and citing the forthcoming papers [9] and [13]. Since the paper
> elsewhere describes making the model publicly available only as
> something that will happen in the future, this leads to the inference
> that authors of the present paper overlap with or are close colleagues
> with the authors of [9] and [13]. If indeed they are (the authorship
> of the paper is still anonymous to me as I write this), this is
> contrary to the desired anonymization. The best way to cite one's own
> work in an anonymous submission is always tricky, and I don't think
> the guidelines that the conference made available make clear a better
> resolution to the dilemma than the one the author(s) chose. A strategy
> the author(s) could consider using if they find themselves in a
> similar situation in the future would be to use phrasing that would
> normally only be used when a third party was distinct, but would also
> be literally true even if they were in fact the same. For instance I
> would have considered it acceptable for the paper to have said "we
> shared an earlier version of our model with the authors of ppcmem2 [9,
> 13]", even if that sharing was with oneself. (This is an exception to
> the baseline principle that misleading literal truths are not
> acceptable in papers, because double-blind reviewers are by that role
> consenting to being kept in the dark about author identities.)
> 
> Some more superficial suggestions, with text [[to delete]] and >to
> add<
> 
> "for example, >some< kernels recognize their own addresses by testing
> whether they encode a negative signed value": Linux/x86-32 with a
> 3GB/1GB split is one common example of a kernel for which this
> approach would not work.
> 
> "Multiple definitions in {\.{o}} files" -> "{\tt .o} files"
> 
> "normally archives can only refer to objects appearing to their left":
> did you mean "appearing to their right"? Or maybe the directionality
> of "refer" is opposite from what I'm expecting? The running example
> doesn't show this because the only archives are inside -( and -), but
> I believe the rule is that undefined symbols are only satisfied by
> archives that are read later, so libraries come after program object
> files and lower-level libraries come after higher-level ones.

Yes, good catch... this should say "archives can only be referenced by objects
appearing to their left". (Note that archives themselves can potentially
reference to the left *or* right.)

> It's confusing that some parts of the paper seem to use "object" to
> mean a piece of data in memory, whereas elsewhere it seems to be used
> as a shorthand for what is first introduced as an "object
> file". Though I can see that it sounds a bit weird to talk about
> archive members as "object files" if they aren't actually separate
> files, I think just using "object" both ways is too ambiguous.
> 
> "Since the GNU linker does not currently provide any way to disable
> these optimisations (even when supplying options intended to disable
> optimisations)[[.]]>,< we diverge from it here"
> 
> "This makes a linker's >role< in `programming in the large'
> comparable"
> 
> "link-time reasoning near the machine level, perhaps following
> Balakrishnan >and Reps< [2]": [2] is a long paper that introduces a
> variety of techniques, so I think it would be good to be more specific
> about what you're thinking of here.
> 
> ===========================================================================
>                             PLDI '16 Review #3B
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: Y. Knowledgeable
>                       Overall merit: B. OK paper, but I will not champion
>                                         it
> 
>                          ===== Paper summary =====
> 
> This paper presents an in-depth discussion of the semantics of
> linking. The authors have created an executable model of linking, in
> Lem and with Isabelle/HOL definitions and handwritten termination
> proofs. The paper itself contains an overview of linking use cases,
> including stages and corner cases, with various small examples strewn
> throughout.
> 
>                       ===== Comments for author =====
> 
> I appreciated that this paper presents an in-depth examination of
> linking. A formalization of this important phase in generating
> binaries is long overdue, and overall I enjoyed reading the paper.
> 
> The main thing missing from this paper is insights. The descriptive
> aspects can already be found in public documentation (although it is
> nice to have them collated and summarized here).  What did we learn
> from this whole exercise? We already knew that linking is messy. The
> high-level vision is somewhat missing from this paper. What is a
> reasonable model for linking in the future? How does this paper get us
> there?

okay, but whole other paper

> The paper refers to an ELF "model" and "formalization", but such a
> formalization is neither contained in the paper nor (as far as I could
> tell) included as a reference. The "model" in this paper appears to be
> just a description of the HOL implementation's phases, which mirrors a
> real linker's phases. Furthermore, the obscure portions of linking
> that are/are not included in the model are distributed throughout the
> paper. A table or other organizational feature clearly listing what is
> included in the model would help.

By "model" we mean the Lem implementation of ELF and the linker, which we included
as additional material in our submission, along with the extraction of an
Isabelle/HOL version of this model.

> The evaluation (Section 4.1) discusses validating the model against 7k
> binaries. What sources of incompleteness in source specification
> documents were found? Did you update the model to deal with all corner
> cases exposed by these binaries? A table summarizing the results from
> this large validation would make it more clear what exactly
> happened. Also, what bugs did you discover when translating the model
> to Isabelle/HOL?

Most ELF binaries found "in the wild" contain section types, symbol types,
relocation types, and so on that are not mentioned explicitly in any formal or
semi-formal document.  Examples of this type include many GNU-specific extensions,
especially those related to prelinking.  These are not described in the Linux
standard base document (LSB) which does partially describe many other GNU-specific
extensions one may find in a typical ELF binary, such as GNU HASH, etc, though
does not appear to have been updated recently.

The ELF model was updated to accommodate these features as they were found during
validation.  Note, we therefore do not claim that our model is comprehensive ---
validating against e.g. MIPS binaries, or even PowerPC binaries produced by an
obscure compiler, may reveal yet more sources of incompleteness that will need to
be added to the formal model on as-needed basis.

Note, there were other cases where the ELF specification is not clear or ambiguous.
One particular (but boring) example was a series of PowerPC64 binaries that
contained a zero-sized segment whose offset was beyond the end of the file.  Trying
to read beyond the end of the list of bytes representing the file caused an exception
when validating --- segment size must be checked first to ensure the segment in
question has a strictly positive size before attempting to read it, yet this is
not immediately apparent from the specification documents and it seemed implied
that one could check for offset before size.

> On a related note, this paper is very grounded in current
> implementation details of linkers -- but that implementation may
> change. How does that affect the results? Why formalize all the warts
> in the linker instead of fixing some of them?

can't just "fix the linker" -- its behaviour is a contract shared between
many compilers and linkers, so would have to "fix" all of them

> Small notes:
> * pg. 2: "broad range number of" ==> "broad range of"
> * pg. 3: "operating-specific" ==> "os-specific" or "operating-system-specific"
> * pg. 9: "we diverge from it here" ==> "We diverge from it here" 
> * pg. 10 "we filed a bug on the gold linter inserting" ==> "we filed a bug on the gold linter for inserting"
> * pg. 11 "This makes a linker's in" ==> "This makes a linker in"
> 
> ===========================================================================
>                             PLDI '16 Review #3C
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: B. OK paper, but I will not champion
>                                         it
> 
>                          ===== Paper summary =====
> 
> The paper describes how to formalize ELF and its linking and to what
> extend this is possible.
> 
> The paper is informally written, without lots of figures and
> formalized semantics; these exist as a separate Lem-Isabell/HOL model,
> submitted as an attachment. The text is mainly concerned with the many
> details of actual linking of ELF files using the system linker on
> Linux/Android. Each linker feature is presented and the authors
> describe how they managed to deal with it in their formal model. Some
> of the features are described for completeness' sake, while others
> seem to cause issues that had to be carefully modelled.
> 
> The ELF model is validated by testing: 7054 real binaries are passed
> through it, analyzed and blitted again to binary code. Each binary is
> analyzed with a readelf-clone of the authors using the ELF model and
> its output is compared to the output of standard readelf, to ensure
> that the information in the ELF file is correctly captured. Also, the
> extracted information is then written again as a binary, which is
> compared byte-for-byte with the original binary, to check if they are
> the same. Finally, extraction to Isabelle/HOL and proofs, found some
> more bugs in the ELF model.

(seems to ignore [other] actual contributions!)

> The linking model is more difficult to validate: linker behavior
> contains non-deterministic computations and cannot be fully captured
> by the proposed model. This means that a standard C library cannot be
> currently linked using the model. However, the authors show that
> hello-world.c can be compiled and successfully linked against the more
> lightweight uclibc. The test mostly succeeds, the only difference
> against ld being that ld has further optimized some instructions and
> the size of the GOT.

non-det and glibc issues are orthogonal (missing IRELATIVE support is the
main reason not to link with glibc right now)

> The authors conclude with related work and some future directions.
> 
>                       ===== Comments for author =====
> 
> This is a well-written paper that describes the experience in
> formalizing ELF and its linking for the Linux platform. The actual
> model used is not really seen in the text, which is more concerned
> with the many obstacles that were encountered during formalization,
> because of the current state of ELF linking semantics and
> implementation.

This was a conscious choice on our part.  This type of modelling of real-world
systems does not lead to neat, easily digestible definitions and specifications
that fit nicely into a paper of this form, like e.g. typing rules of calculi or
programming languages.  As a result, we thought it better to eschew filling the
paper with definitions and concentrate our effort on trying to highlight *why*
the definitions are like this.

> This seems like a step forward in the field of compiler
> verification. It examines the many details that we take for granted
> when we assume the linker works and, most important, gives out a list
> of the non-deterministic features of the Linux system linker. This
> last thing means that this work reaches the point where progress is no
> longer a matter of engineering but of better specification of linking.
> 
> I have the following remarks:
> 
> 1. How difficult are the termination proofs? The whole effort seems
> big enough for any attempt at proving things via a proof assistant
> being painful. Any remarks on that?

The termination proofs in Isabelle/HOL were not especially onerous (modulo working
around bugs in Isabelle itself, as mentioned above).  Their size
is more a reflection of the size of the formalisation itself, which is quite large,
consisting of a large number of separate definitions and functions that all must
be proved to be terminating.  Note also that, as mentioned above, a lot of the
complex functions that actually perform the link (in link.lem) are not recursive,
and make use of higher-order functions like folds and filters, and therefore are
terminating by fiat.

The reviewer is correct in saying that proving anything non-trivial using this
formalisation will be a significant amount of work, hence our comments at the
start of the rebuttal, though given recent large formalisation projects (e.g.
CompCert, seL4, CerCo, and so on) this is perhaps to be expected for any new
formalisation project.

> 2. What kinds of bugs did the extraction/proving in Isabelle/HOL find?
> Why weren't they captured by testing? Were they different in nature?

We have answered an identical question above.

> 3. Why are the proofs also done in Coq/HOL4? Isn't one proof assistant
> enough? Is this related to integration with CakeML or other formalized
> compilers?

Unfortunately, the people who would be amongst those most interested in a formal
model of linking (i.e. the formal verification community) have not settled on a
single theorem prover to rally around.  We have CompCert written in Coq, CakeML
written in HOL4, seL4 written in Isabelle/HOL, and so on, and we envisage that
our work will be at least interesting to people involved in all of these projects.
As a result, to maximise the utility of our model, we decided to try to extract
our definitions to all theorem prover backends that the Lem tool supports, or at
least plan to.

Note, here: the Isabelle/HOL extraction is the only extraction at present that
includes both definitions and termination proofs for recursive functions (hence
the generation of simplification rules associated with those functions).  The
HOL4 extraction consists of definitions only, as our model unearthed a fairly
serious bug in the termination mechanism of HOL4 that is yet to be fixed but
prevents us from completing proofs of termination for recursive, monadic functions.
(A tentative fix by the HOL4 team was pushed but then reverted).  A Coq extraction
is only planned at this stage, and is not yet anything concrete.

> 4. (Possibly related to the previous remark.) How easy is it for a
> "verified" linker like the one proposed to be interfaced with a
> verified C/C++ compiler? In other words, how easy is it for the
> linking specification (that the linker follows) to be combined with a
> C/C++ spec (that e.g. CompCert follows)? Does this require using the
> same (e.g. Coq) language/infrastructure?

This is another paper's worth of work (at least!) and at the moment we have no
true idea of how hard this work would be (perhaps another PLDI paper? certainly
not trivial).  Note that we have explicitly listed this---the extension of
existing verified compilers like CompCert and CakeML---as possible further work
and is certainly something that we are interested in.

> 5. Although the text is good, the reader is left with few technical
> insights as to how to write such a model (apart from reading the
> attached code). It feels strange that such a seemingly big and complex
> piece of work does not demonstrate its technical "guts". More
> technical stuff (like figures 2 and 3, the "symbolic memory images" of
> section 5.1 or the "interpreter" and "eligibility predicate" of
> section 5.2) would be welcome, to give a flavor of the way that such a
> model can be formulated. Were there any expressive power problems
> writing the ELF specification? Did the authors need any clever hacks
> or principled programming to manage the specs of ELF and linking?

We certainly ran into difficulties with the Lem language; our specification was
the biggest Lem codebase yet, by some distance, and revealed several problems.
For example, abstracting over 32- and 64-bit ELF details is not possible in Lem,
so much code is duplicated and we effectively use the 64-bit definitions as our
"internal representation" in many places. Limitations of Lem typeclasses and
lack of support for cyclic references were also problems, necessitating
hackarounds; we could easily add a brief discussion of these. Similarly, we
could add more snippets of the "guts" of the spec, although space is lacking.

> 6. The paper is not about modules at the level of the source at all;
> it is about object code and its linking in a specific setting: ELF
> linking on a Linux/Android platform. Since it is language-agnostic, it
> seems that some of the higher-level related work should be seen as
> complementary to it. Is this the case? Can this work be combined with
> any of the related work?

Certainly, formally specifying how source language implementations map down to
linker-speak is worthwhile, as we noted in section 7. Doing so would be a
complementary but separate (and large) piece of work, and would necessarily be
specific to specific language implementations and/or ABIs.

mention Kayvan's work (+ others' similar)?

> 7. As a note, this work is also an important step in the direction of
> binary-identical reproducible builds as it demonstrates the looseness
> that must be eliminated from the linking stage (section 5.3).
> 
> 8. Typos: "a single single" (p. 3), "Multiple definitions in \dot{o}
> files" (p. 5), "pre-exi[s]ting output file" (p. 9), "apply
> optimisations [to] the relocations" (p. 9).
> 
> ===========================================================================
>                             PLDI '16 Review #3D
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: D. Reject
> 
>                          ===== Paper summary =====
> 
> This paper describes a "formal model" for a realistic linker for ELF
> files, written in the modeling language Lem. The model consists of
> 15,000 lines of Lem for specifying ELF, linking generally, and ABI
> specific details. Extraction to Isabelle/HOL has enabled the authors
> to also perform some theory, e.g., termination proofs of recursive
> linking. The Lem model is executable (Lem extracts to OCaml IIRC), and
> so has been validated by running it on thousands of binaries (and used
> as a front end for at least one other tool).
> 
> The paper also describes the (perhaps surprising) ways that
> linker-based functionality is employed in modern development, and how
> its directives sometimes bubble into the language but are not
> specified as part of the language semantics. The goal of the work is
> to reveal some of the magic that happens below the level of the
> language, and the compiler. Ultimately, the authors hope that their
> model will serve as a foundation for future work, such as metatheory
> about linking, it's use in a verified compiler, etc.
> 
>                       ===== Comments for author =====
> 
> The goal of developing a formal model to understand modern linking
> seems important, and the proposed applications of such a model listed
> on p. 11 seem interesting (e.g., avoiding using ld altogether but
> producing verified binaries directy from a compiler). The produced
> artifact is impressive, and the evaluation of it to show that it is
> accurate is convincing.
> 
> But the paper itself is not particularly illuminating, so I do not
> recommend acceptance. I think there are two problems here.
> 
> First, the paper says very little about the substance of the model. It
> presents a long list of surprising and disturbing features that have
> crept into modern linkers. It presents some meta information about how
> its ELF model was evaluated (running it on binaries) but not much
> about the model itself. It describes, prosaically for a given example,
> the stages of static linking, as implemented by the model (and actual
> linkers, presumably). And it points out that the model, extracted, can
> perform real work. But I expected the authors to teach me something
> interesting, e.g., the "essence" of linking, perhaps, in a way that is
> possibly more realistic or relevant than prior idealized
> presentations. At the end of the paper, I had basically learned that
> linking is an arcane activity perhaps in need of a re-think. I did not
> have any larger insight than I did when I started.

no simple "essence"; that's the point

verified compilation needs to work! to work with real, existing code,
working with existing linker features is necessary. hence our choice to
model the arcane stuff as-is

> Second, there is no substantial use of the artifact that is reported
> in any detail, and hence the model's true purpose is not really
> evaluated. There are many ways that one could have modeled
> linking. The authors made particular design choices. How do we know
> that their design of the model is an effective one? Running it is not
> enough to evaluate it. I could have rewritten ld in OCaml and run it
> on a bunch of binaries; I don't think we'd call that worthy of a PLDI
> paper. Instead, the whole point of a formal model is proving things
> with it. To show that the model truly facilitates proof, we have to
> prove something. Poor design decisions might get in the way of
> effective proofs. But no serious applicatiosn of the model have been
> carried out; only some termination proofs for recursive linking that
> the paper barely reports on.

We see formal proof as only one point of formalisation.  Another is the
ability to precisely communicate with external engineers, companies (e.g. ARM),
in a precise manner about complex mechanisms and artefacts at a level of abstraction
above a C implementation.  We also see formalisation as a process that enables
lots of other research (not necessarily formal proof) and helps clarify for the
benefit of the rest of the community that which is being formalised.

Further, proving anything remotely interesting about a model of this scale is
another substantial paper's worth of work, at least.  We of course plan in the
near future to start proving things using our model, and mention many ideas along
these lines in our "future work" section, but there's limits in what can be done
effectively within the confines of a single PLDI paper.

> In short, this is impressive work that will surely be useful. But I
> don't see much value in this particular paper about it, just yet.
> 
> Other comments/questions/nits:
> 
> P. 1: You say that a large fraction of code bases rely on linker
> functionality; what is your basis for this claim? Clearly all programs
> rely on linking, since all sizeable programs require separate
> compilation. Programs that use libc rely on the wacky way that libc
> has evolved with linking; but how many programs rely on non-obvious
> linker features directly (e.g., require more than the "default
> script")? I didn't see any argumentation about this; at best, "low
> level" code like OSs might need it, but I wouldn't call this a large
> fraction of all codebases.

It is true that few codebases use non-default linker scripts. However, the other
linker features mentioned in section 3 are much more commonly used. The "large
fraction" of codebases certainly includes most large libraries, which have to
grapple with symbol visibility (viz. Drepper's guide for library authors) and
often also with versioning. Besides libraries, it is not uncommon for large
application codebases to ship with wrapper scripts that play tricks with
LD_PRELOAD or LD_LIBRARY_PATH (e.g. Adobe Reader or Google Chrome both do thiss,
off the top of our heads). Furthermore, any C++ codebase is dependent on
link-time uniqueness handling. User-level pthreads applications fairly often
make use of weak references. Code featuring plug-in systems uses the dynamic
linking API (dlopen() etc.). And so on. Although no one feature is common,
almost no large library, and few large applications, get away without using any
of these features.

> P. 1: Likewise, you say linker speak is used haphazardly and often
> incorrectly -- what's your basis for this claim?

In the text, "haphazardly" refers to the *specification* of linker-speak, not
its usage. We do say that linker-speak is often used "incorrectly or... in
unportable or fragile ways". Section 3 already contains several anecdotes to
this effect: we note how substitution via LD_PRELOAD is inherently unsound; how
link-time interposition (with --wrap) fails to capture all references; how an
often-recommended option (-fvisibility=hidden) breaks the source semantics of 
C++. A further example is that clients of dlsym() are generally not robust to
symbol versioning (for which the only robust interface, dlvsym(), is
non-portable). Our point is not that the absolute frequency of erroneous usage
is high relative to other programming practices, but that linker-speak is very
difficult to use robustly, hence worthy of machine-assisted reasoning about.

> P. 1: The code example with "alias" annotations is interesting
> motivation, showing how linker speak bubbles into the main
> language. But IIRC this example does not return, e.g., when explaining
> your formal model; you instead focus on hello world. It would have
> been nice to close the loop on this.

Linking a hello-world program involves *all* the C library code that we show in
section 3 (or close equivalents). For example, many linker aliases (coming from
libc) are processed when linking hello-world. We choose hello-world precisely
because although it is a small/simple program, linking it is *not* at all a
simple link job.

> P. 2: I'm surprised that you have not cited Levine's "Linkers and
> Loaders" book as a careful reference.
> 
> P. 5: "multiple defintions of È¯ files" --> "multiple definitions of .o
> files"
> 
> P. 7: I found 4.1 hard to understand at first: Did you just use your
> ELF model to parse a bunch of binaries? The first paragraph doesn't
> say much; the next two describe the use of readelf and hexdump
> equivalents whose output is diffed -- is this what you mean by
> "testing?"

We parsed binaries using an OCaml version of our Lem model, extracted by Lem
itself, and wrote a tool around this code to mimic the output of readelf when
passed flags like --section-headers, --file-header, --relocs, and so on, using
information extracted from the parsed binaries by the Lem model.  We then mimicked
the formatting and output of readelf's output and compared the output on ~7000
binaries automatically using a script and a diff tool.

We also separately used hexdump to compare the byte-for-byte output of our code
when a binary was read in and then blitted back out, again using diff, to ensure
that our parsing and blitting code was correct.

> 
> P. 8: I didn't follow the paragraph at the end of 5.1 on symbolic
> memory ranges. Footnote 6 was also confusing: -IX options might be
> linked symbolically? What does it mean to link an option?

The point here is that "options" of the form "-lX" actually denote libraries
(e.g. -lc is mapped to /usr/lib/libc.a or similar). It is unfortunate that the
typeface makes 'l' appear like 'I'.

> P. 9 "Our formalisation defines the linker script language's abstract
> syntax in Lem." What does this mean? Do you mean that you hand
> translated the script to what you believed to be an equivalent
> representation in the Lem model? Or, do you have a parser, and that
> this is what it produced, in Lem syntax, for the default script?

The former: for now, we hand-translate the script into our own representation.

> P. 11: There's some grammatical issue in the sentence 'This makes a
> linker's in "programming the large" comparable ...'
> 
> ===========================================================================
>                             PLDI '16 Review #3E
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: A. Good paper, I will champion it
> 
>                          ===== Paper summary =====
> 
> This paper describes the complexities of formally modeling the linker
> for the x86 variety of ABI. The work provides a specification in Lem
> and in Isabelle/HOL.  The paper notes many real-world difficulties in
> modeling linker-speak.
> 
>                       ===== Comments for author =====
> 
> Pros:
> 
>  + Linking is a critical aspect to consider for formal guarantees to
>  be accurate, yet  ignored by most formal analysis. This paper helps
>  change that.
> 
>  + The paper articulates a wide variety of implementation-dependent
>  and/or obscure facts of linking.
> 
>  + The paper says the implementation produced by the specification is
>  already being used by other projects, and has been used to check real
>  executables.
> 
> 
> Cons:
> 
>  - No concrete evaluation presented. The authors discuss checking the
>    linker with hello_world and other programs, but I felt as though
>    the "tool" aspect of the paper was shallow.
> 
>  - Linkers have many implementation-dependent components, and it's
>    unclear whether making a formal specification for one is that
>    revolutionary.
> 
>  - There were no insights in how to create a better formal linker
>    standard.  This is my main comment: I would like to have had more
>    lessons learned on what would be a "clean" specification.  The
>    paper described many implementation-dependent and obscure features,
>    but it wasn't clear that there could be a cleaner specification.

(as above; this wouldn't necessarily help anyone)

> 
> Commentary:
> In a sense, this paper is trying to live in between two worlds.  On
> the one hand, we have the implementation is the specification often
> found in compilation (not just here, but also in parsing for example).
> On the other, we have the PL-theory viewpoint that a formal
> specification outside implementation is important.
> 
> The authors discuss implementation-specific behaviors, and at one
> point, do say that differing implementations are at odds (S 5.1). This
> reminded me that BFD is really, really buggy (at least the parts I
> look at).  Did you end up formalizing bugs?  If it's implementation
> specific behavior that you already have competing definitions for, why
> is the formalization important?

hard to define "bug" once behaviour is commonplace.  Still want to
capture looseness, i.e. "any legal implementation", not just specific. We
also did comparisons with GNU gold (see section XX; reported a bug), i.e.
not just BFD.

> I found the authors did a good job describing the intricacies of
> linking.  I ended up walking away with the impression "wow, that
> sounds complex", but I wasn't sure how widely applicable outside
> formally documenting BFD the results would be.

again, not BFD.

> I would love to see more text devoted to "what a clean linker would
> look like".  I think quite a bit of the future work section could be
> omitted. It's interesting to read, but I don't feel it adds scientific
> weight to the paper.

(similar comments)

> Overall, I think this paper does a great job of bringing science to
> some of the engineering/compiler internals process that adds to our
> understanding. While I would have liked to see them go beyond the
> single formal model and explore other areas (e.g., are there places in
> the linking that are tricky for no good reason which the formalism
> helps us reason about?), I think there is obvious limits to what can
> be in one paper.

yes, there are -- thank you

> I hope the source is indeed released so others can benefit.  My score
> is probably a 0.5 on the anticipation it will be made available.

certainly

> Small things:
> 
>  - I'm not convinced your characterization of ELF is the standard is
>    true.  I think focusing on ELF is great, and it is a very widely
>    used standard and inspiration (along with it's predecessors) for a
>    great many formats.  However, I felt there was not enough attention
>    paid to different OS linker differences, esp. ELF vs. Microsoft
>    with PE.

We are careful not to proclaim ELF as *the* standard in Section 2.  Rather, we
claim that ELF is the de facto standard executable and linkable format on GNU/Linux
and BSD and related operating systems (which it is).  Further, we acknowledge
that Windows and MacOS each have their own executable and linkable formats in
PE and Mach-O that whilst incompatible either have broadly similar features to
ELF.

Certainly, we can envisage that similar work specifying PE, a.out, Mach-O, etc.
linking is possible in analogy with this work.

>  - I wrote "Meh" next to bullet point one for contributions.  I find
>    this sort of contribution underwhelming. It's great for a blog; why
>    for a scientific paper that should advance the field? I'm guessing
>    there is a community of compiler developers who know many if not
>    all these things.  I would leave this off the contribution list as
>    it's a contribution of the discussion, but not really a
>    contribution to science.

On the contrary, we believe that our conception of linking in terms of
"linker-speak" is novel, and particularly valuable since it is *not* currently
shared by compiler authors. We have anecdotal evidence that even many compiler
developers have only a weak understanding of linking -- particularly in research
communities. (Although we removed them for brevity and anonymity reasons, we can
point to reported bugs in both the OCaml and GHC which illustrate this.)

>  - 5.1 contrasts deterministic with non-deterministic.  However, I
>    think you mean implementation-specific (the word looseness also
>    used is good).  I would take non-deterministic to mean there is a
>    coin flip somewhere, which doesn't seem the case here.  In other
>    words, I would prefer not using the word non-deterministic; it
>    seems to be the wrong word.

In fact, a linker *would* be allowed to resolve looseness randomly, and this is
not even a completely silly idea. For example, a hypothetical security-hardening
linker might randomise the order of certain input objects in the link, to make
address assignments less predictable. Our specification allows us uniformly to
say what variation is and isn't allowed, whether between different linkers or
between runs of the same linker.

>  - S5.2 first says 891 objects, then 897 objects. I was confused.

This is correct; there are 891 objects in archives, and 6 named directly on the
command line shown, giving 897 in total.

>  - Intro: "Previous semantic work on linking..." you should have a
>    citation to which previous work you are referring.
> 
>  - S3.2: "libary" -> "library"
> 
>  - You appear to have de-anonymized with references.
> 
> 
> ==-==    following in your review: Is the paper well-motivated? What
> ==-==    problem does it address, and is it an important problem? Does the
> ==-==    paper significantly advance the state of the art or break new
> ==-==    ground? What are the paperâs key insights? What are the paperâs
> ==-==    key scientific and technical contributions? Does the paper
> ==-==    credibly support its claimed contributions? What did you learn
> ==-==    from the paper? Is the paper sufficiently clear that most PLDI
> ==-==    attendees will be able to read and understand it? Does the paper
> ==-==    clearly establish its context with respect to prior work? Does it
> ==-==    discuss prior work accurately and completely? What impact is this
> ==-==    paper likely to have (on theory & practice)?
> 
