Rejected · You are an author of this paper.
	RevExp	OveMer
Review #3A		X	C
Review #3B		Y	B
Review #3C		X	B
Review #3D		X	C
Review #3E		X	B



===========================================================================
                            PLDI '16 Review #3A
                     Updated 17 Jan 2016 9:43:10am EST
---------------------------------------------------------------------------
  Paper #3: The missing link: explaining ELF static linking, semantically
---------------------------------------------------------------------------

                 Reviewer expertise: X. Expert
                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it

                         ===== Paper summary =====

This paper argues that complete and correct compilation and program
analysis requires a more formal representation and understanding of
linking. In support of this goal, it reimplements a large subset of an
ELF linker in the style of GNU ld, in a way that goes easily into the
Isabelle/HOL proof assistant. As motivation, the paper gives examples
of several subtleties of ELF linking, including some features which
change or break the otherwise-standard semantics of C. The paper's
specification of the ELF file format is verified by I/O comparison
experiments versus GNU readelf over a large corpus of binaries. The
paper documents a variety of sources of non-determinism in linking,
and describes an approach of supplementing a non-deterministic
specification with a "personality function" to emulate the choices of
a particular existing implementation, but the personality function for
GNU ld is not yet accurate enough to produce matching outputs for
realistic-sized examples. The model linker is however able to produce
a working hello-world binary when linking with uClibc. As first steps
in using the model for formal reasoning, the author(s) have supplied
termination proofs to allow the ELF file format model to be used in
Isabelle/HOL, and stated in Isabelle/HOL but not proved a correctness
property of x64 relocation in terms of the instruction semantics. The
paper ends by laying out a variety of directions for future working
building on a precise understanding of linkers, including improving
specifications, program analysis, verified compilation, and
improvements to the linking interface.

                      ===== Comments for author =====

Key strengths and weaknesses:

+ The paper provides a good overview of linker features

+ The paper demonstrates that linking is important to program
semantics, but complex and poorly specified

+ The specification approach seems clean, supporting both execution
and use in proofs

- The model linker seems to yet scale only to small programs and a C
library that, while large, is not the standard Linux one

- No interesting theorems have yet been proved

I found this paper fun and easy to read, and it serves well as a
survey of surprisingly-complex linker mechanisms and as a position
paper on why a more formal understanding of them would be worthwhile.
However the actual formalization work that I would have expected to be
the core of the paper feels preliminary and is not so impressive.
Modeling a linker without proving anything about it is basically just
reimplementing a linker using a restrictive programming language.  The
level of detail the paper gives about how it does this makes it sound
well-structured and suitable as a basis for future work, but a real
demonstration of its value would be by constructing proofs. The
theorem about relocation correctness that the paper describes
formulating would sounds like a good demonstration, but if the
author(s) haven't yet proved it, we don't have much confidence that
its statement is correct. This feels like a worthwhile project, so I'm
hoping that what happened is just that the PLDI submission deadline
came too soon. It sounds like with a few more compatibility changes
and bugfixes the model linker could be a robust tool, and if the
author(s) can prove some basic facts about it I think the value of the
formalization would be more clear.

I'd missed the distinction on a first reading, but on looking back,
the section structure seems to suggest that the termination proofs
apply only to the model of the ELF file format, and not to the model
linker. On the other hand the mention in the introduction does not
make this distinction, so I guess I'd say the paper is ambiguous. By
analogy, are the 1500 lines of handwritten termination proofs are
roughly proving the termination of ELF file parsing as in "readelf",
or the termination of linking as in "ld"? The latter would be a
somewhat more interesting result.

The clearest example I understood from the paper about the behavior of
the linker breaking what should be C language guarantees is the
example of references to weak symbols being null. The case for the
practical importance of the paper's work would be stronger, though, if
the paper could give a more pernicious example of linking causing
undesirable behavior, such as something silently incorrect, or clearly
contrary to programmer intent. Perhaps this position could be filled
by expanding on the claim in the introduction about Figure 1 that the
calls to _int_malloc might not go the function defined in that figure?
It wasn't clear to me which linker behavior could be the cause of
that.

I would have been curious to hear a bit more about the further bugs in
the ELF specification that were revealed not by testing on 7k binaries
but by the Isabelle/HOL proof process for termination and "various
other lemmas".

I also feel like I should comment a bit on anonymization. The
author(s) removed identifying information in several places, but I
think they hurt likely their degree of anonymity by mentioning that
the model described in this paper has already been used in the ppcmem2
system and citing the forthcoming papers [9] and [13]. Since the paper
elsewhere describes making the model publicly available only as
something that will happen in the future, this leads to the inference
that authors of the present paper overlap with or are close colleagues
with the authors of [9] and [13]. If indeed they are (the authorship
of the paper is still anonymous to me as I write this), this is
contrary to the desired anonymization. The best way to cite one's own
work in an anonymous submission is always tricky, and I don't think
the guidelines that the conference made available make clear a better
resolution to the dilemma than the one the author(s) chose. A strategy
the author(s) could consider using if they find themselves in a
similar situation in the future would be to use phrasing that would
normally only be used when a third party was distinct, but would also
be literally true even if they were in fact the same. For instance I
would have considered it acceptable for the paper to have said "we
shared an earlier version of our model with the authors of ppcmem2 [9,
13]", even if that sharing was with oneself. (This is an exception to
the baseline principle that misleading literal truths are not
acceptable in papers, because double-blind reviewers are by that role
consenting to being kept in the dark about author identities.)

Some more superficial suggestions, with text [[to delete]] and >to
add<

"for example, >some< kernels recognize their own addresses by testing
whether they encode a negative signed value": Linux/x86-32 with a
3GB/1GB split is one common example of a kernel for which this
approach would not work.

"Multiple definitions in {\.{o}} files" -> "{\tt .o} files"

"normally archives can only refer to objects appearing to their left":
did you mean "appearing to their right"? Or maybe the directionality
of "refer" is opposite from what I'm expecting? The running example
doesn't show this because the only archives are inside -( and -), but
I believe the rule is that undefined symbols are only satisfied by
archives that are read later, so libraries come after program object
files and lower-level libraries come after higher-level ones.

It's confusing that some parts of the paper seem to use "object" to
mean a piece of data in memory, whereas elsewhere it seems to be used
as a shorthand for what is first introduced as an "object
file". Though I can see that it sounds a bit weird to talk about
archive members as "object files" if they aren't actually separate
files, I think just using "object" both ways is too ambiguous.

"Since the GNU linker does not currently provide any way to disable
these optimisations (even when supplying options intended to disable
optimisations)[[.]]>,< we diverge from it here"

"This makes a linker's >role< in `programming in the large'
comparable"

"link-time reasoning near the machine level, perhaps following
Balakrishnan >and Reps< [2]": [2] is a long paper that introduces a
variety of techniques, so I think it would be good to be more specific
about what you're thinking of here.

ADDED AFTER AUTHOR RESPONSE:

To a certain extent I sympathize with the author(s)' position: it is
frustrating to put a lot of work into a project, in an area the
reviewers all seem to agree is important and worthwhile, and then for
the paper to get such a mixed reception. But of course papers aren't
evaluated primarily on the effort that goes into them, but on the
contributions they demonstrate. And it still feels to me that the
paper fails to demonstrate enough of a contribution from the work that
it has done.

Machine-checked proof is the first contribution that seemed obvious to
me from what the paper has done, but I agree that it need not be the
only possible one. If the model in its executable form were a more
practical tool that could be pragmatically compared with real linkers
(e.g. as a test oracle, as the response mentions), that would be one
direction. If the modeling effort had taught you reusable lessons
about the modeling process, or provided insights about how linking
could be simplified, those might have also served to show the value of
the work. Perhaps with the benefit of hindsight it would have been
better to pick a particular direction to evaluate first, and to focus
the development more there. Right now it feels to me like the paper
has a lot of promise about future value, but it's lacking anything
that can be demonstrated now.

I can certainly see that the large-scale proofs that would be the
ultimate goal, like combining linking with a certified compiler, could
easily be a multi-person-year project. But I'd be curious what subset
of the proof about relocation the paper mentions would reach that
level of "substantial" or "interesting" that would be an infeasible
amount of extra work. The limited version for a single relocation type
and a single instruction that the paper was able to state sounded to
me like it would have been a much more reasonable first step.

I agree that the discussion in Section 3.2 uses "malloc" as an
example, but what it felt like the paper set me up to expect and then
disappointed was an example where _int_malloc got redirected as an
effect of something a programmer might plausibly do.

I appreciate the discussion of the problems found respectively by
concrete testing and early proof work.

===========================================================================
                            PLDI '16 Review #3B
                     Updated 17 Jan 2016 2:06:07pm EST
---------------------------------------------------------------------------
  Paper #3: The missing link: explaining ELF static linking, semantically
---------------------------------------------------------------------------

                 Reviewer expertise: Y. Knowledgeable
                      Overall merit: B. OK paper, but I will not champion
                                        it

                         ===== Paper summary =====

This paper presents an in-depth discussion of the semantics of linking. The authors have created an executable model of linking, in Lem and with Isabelle/HOL definitions and handwritten termination proofs. The paper itself contains an overview of linking use cases, including stages and corner cases, with various small examples strewn throughout.

                      ===== Comments for author =====

I appreciated that this paper presents an in-depth examination of linking. A formalization of this important phase in generating binaries is long overdue, and overall I enjoyed reading the paper. 

The main thing missing from this paper is insights. The descriptive aspects can already be found in public documentation (although it is nice to have them collated and summarized here).  What did we learn from this whole exercise? We already knew that linking is messy. The high-level vision is somewhat missing from this paper. What is a reasonable model for linking in the future? How does this paper get us there?

The paper refers to an ELF "model" and "formalization", but such a formalization is neither contained in the paper nor (as far as I could tell) included as a reference. The "model" in this paper appears to be just a description of the HOL implementation's phases, which mirrors a real linker's phases. Furthermore, the obscure portions of linking that are/are not included in the model are distributed throughout the paper. A table or other organizational feature clearly listing what is included in the model would help.

The evaluation (Section 4.1) discusses validating the model against 7k binaries. What sources of incompleteness in source specification documents were found? Did you update the model to deal with all corner cases exposed by these binaries? A table summarizing the results from this large validation would make it more clear what exactly happened. Also, what bugs did you discover when translating the model to Isabelle/HOL? 

On a related note, this paper is very grounded in current implementation details of linkers -- but that implementation may change. How does that affect the results? Why formalize all the warts in the linker instead of fixing some of them? 

Small notes:
* pg. 2: "broad range number of" ==> "broad range of"
* pg. 3: "operating-specific" ==> "os-specific" or "operating-system-specific"
* pg. 9: "we diverge from it here" ==> "We diverge from it here" 
* pg. 10 "we filed a bug on the gold linter inserting" ==> "we filed a bug on the gold linter for inserting"
* pg. 11 "This makes a linker's in" ==> "This makes a linker in"

ADDED AFTER AUTHOR RESPONSE

I am still weakly in favor of this paper. However, I was not very excited to see that apparently there were no lessons learned (aside from that linking is ad hoc) and it would take years of work to draw any conclusions from this paper. I think it would not have been too much work to synthesize and discuss linking a bit more.

===========================================================================
                            PLDI '16 Review #3C
                     Updated 15 Jan 2016 8:15:29am EST
---------------------------------------------------------------------------
  Paper #3: The missing link: explaining ELF static linking, semantically
---------------------------------------------------------------------------

                 Reviewer expertise: X. Expert
                      Overall merit: B. OK paper, but I will not champion
                                        it

                         ===== Paper summary =====

The paper describes how to formalize ELF and its linking and to what extend this is possible.

The paper is informally written, without lots of figures and formalized semantics; these exist as a separate Lem-Isabell/HOL model, submitted as an attachment. The text is mainly concerned with the many details of actual linking of ELF files using the system linker on Linux/Android. Each linker feature is presented and the authors describe how they managed to deal with it in their formal model. Some of the features are described for completeness' sake, while others seem to cause issues that had to be carefully modelled.

The ELF model is validated by testing: 7054 real binaries are passed through it, analyzed and blitted again to binary code. Each binary is analyzed with a readelf-clone of the authors using the ELF model and its output is compared to the output of standard readelf, to ensure that the information in the ELF file is correctly captured. Also, the extracted information is then written again as a binary, which is compared byte-for-byte with the original binary, to check if they are the same. Finally, extraction to Isabelle/HOL and proofs, found some more bugs in the ELF model.

The linking model is more difficult to validate: linker behavior contains non-deterministic computations and cannot be fully captured by the proposed model. This means that a standard C library cannot be currently linked using the model. However, the authors show that hello-world.c can be compiled and successfully linked against the more lightweight uclibc. The test mostly succeeds, the only difference against ld being that ld has further optimized some instructions and the size of the GOT.

The authors conclude with related work and some future directions.

                      ===== Comments for author =====

This is a well-written paper that describes the experience in formalizing ELF and its linking for the Linux platform. The actual model used is not really seen in the text, which is more concerned with the many obstacles that were encountered during formalization, because of the current state of ELF linking semantics and implementation.

This seems like a step forward in the field of compiler verification. It examines the many details that we take for granted when we assume the linker works and, most important, gives out a list of the non-deterministic features of the Linux system linker. This last thing means that this work reaches the point where progress is no longer a matter of engineering but of better specification of linking.

I have the following remarks:

1. How difficult are the termination proofs? The whole effort seems big enough for any attempt at proving things via a proof assistant being painful. Any remarks on that?

2. What kinds of bugs did the extraction/proving in Isabelle/HOL find? Why weren't they captured by testing? Were they different in nature?

3. Why are the proofs also done in Coq/HOL4? Isn't one proof assistant enough? Is this related to integration with CakeML or other formalized compilers?

4. (Possibly related to the previous remark.) How easy is it for a "verified" linker like the one proposed to be interfaced with a verified C/C++ compiler? In other words, how easy is it for the linking specification (that the linker follows) to be combined with a C/C++ spec (that e.g. CompCert follows)? Does this require using the same (e.g. Coq) language/infrastructure?

5. Although the text is good, the reader is left with few technical insights as to how to write such a model (apart from reading the attached code). It feels strange that such a seemingly big and complex piece of work does not demonstrate its technical "guts". More technical stuff (like figures 2 and 3, the "symbolic memory images" of section 5.1 or the "interpreter" and "eligibility predicate" of section 5.2) would be welcome, to give a flavor of the way that such a model can be formulated. Were there any expressive power problems writing the ELF specification? Did the authors need any clever hacks or principled programming to manage the specs of ELF and linking?

6. The paper is not about modules at the level of the source at all; it is about object code and its linking in a specific setting: ELF linking on a Linux/Android platform. Since it is language-agnostic, it seems that some of the higher-level related work should be seen as complementary to it. Is this the case? Can this work be combined with any of the related work?

7. As a note, this work is also an important step in the direction of binary-identical reproducible builds as it demonstrates the looseness that must be eliminated from the linking stage (section 5.3).

8. Typos: "a single single" (p. 3), "Multiple definitions in \dot{o} files" (p. 5), "pre-exi[s]ting output file" (p. 9), "apply optimisations [to] the relocations" (p. 9).

ADDED AFTER AUTHOR RESPONSE:

The author response was informative and mostly answered my questions. I found the description about how easy it was to write the model in Lem interesting and a small "lesson learned" that should appear somewhere in the paper, even as a brief note.

About a linking "essence" not being there because currently linking is too ad-hoc: if this is true, it is a disappointing fact because then formalization can appear as (higher-level) programming. The authors "see formal proof as only one point of formalisation"; I understand that a formal linking model is also an advance but a complete formal proof is inevitable if we want to eventually have a verified implementation.

===========================================================================
                            PLDI '16 Review #3D
                     Updated 19 Jan 2016 4:49:34pm EST
---------------------------------------------------------------------------
  Paper #3: The missing link: explaining ELF static linking, semantically
---------------------------------------------------------------------------

                 Reviewer expertise: X. Expert
                      Overall merit: C. Weak paper, though I will not fight
                                        strongly against it

                         ===== Paper summary =====

This paper describes a "formal model" for a realistic linker for ELF
files, written in the modeling language Lem. The model consists of
15,000 lines of Lem for specifying ELF, linking generally, and ABI
specific details. Extraction to Isabelle/HOL has enabled the authors
to also perform some theory, e.g., termination proofs of recursive
linking. The Lem model is executable (Lem extracts to OCaml IIRC), and
so has been validated by running it on thousands of binaries (and used
as a front end for at least one other tool).

The paper also describes the (perhaps surprising) ways that
linker-based functionality is employed in modern development, and how
its directives sometimes bubble into the language but are not
specified as part of the language semantics. The goal of the work is
to reveal some of the magic that happens below the level of the
language, and the compiler. Ultimately, the authors hope that their
model will serve as a foundation for future work, such as metatheory
about linking, it's use in a verified compiler, etc.

                      ===== Comments for author =====

The goal of developing a formal model to understand modern linking
seems important, and the proposed applications of such a model listed
on p. 11 seem interesting (e.g., avoiding using ld altogether but
producing verified binaries directy from a compiler). The produced
artifact is impressive, and the evaluation of it to show that it is
accurate is convincing.

But the paper itself is not particularly illuminating, so I do not
recommend acceptance. I think there are two problems here.

First, the paper says very little about the substance of the model. It
presents a long list of surprising and disturbing features that have
crept into modern linkers. It presents some meta information about how
its ELF model was evaluated (running it on binaries) but not much
about the model itself. It describes, prosaically for a given example,
the stages of static linking, as implemented by the model (and actual
linkers, presumably). And it points out that the model, extracted, can
perform real work. But I expected the authors to teach me something
interesting, e.g., the "essence" of linking, perhaps, in a way that is
possibly more realistic or relevant than prior idealized
presentations. At the end of the paper, I had basically learned that
linking is an arcane activity perhaps in need of a re-think. I did not
have any larger insight than I did when I started.

Second, there is no substantial use of the artifact that is reported
in any detail, and hence the model's true purpose is not really
evaluated. There are many ways that one could have modeled
linking. The authors made particular design choices. How do we know
that their design of the model is an effective one? Running it is not
enough to evaluate it. I could have rewritten ld in OCaml and run it
on a bunch of binaries; I don't think we'd call that worthy of a PLDI
paper. Instead, the whole point of a formal model is proving things
with it. To show that the model truly facilitates proof, we have to
prove something. Poor design decisions might get in the way of
effective proofs. But no serious applicatiosn of the model have been
carried out; only some termination proofs for recursive linking that
the paper barely reports on.

In short, this is impressive work that will surely be useful. But I
don't see much value in this particular paper about it, just yet.

==== ADDED AFTER AUTHOR RESPONSE ====

In response to my concerns, the authors wrote:

*We see formal proof as only one point of formalisation. Another is the ability to precisely communicate with external engineers and systems companies, in a precise manner about complex mechanisms and artefacts at a level of abstraction above a C implementation. We also see formalisation as a process that enables lots of other research (not necessarily formal proof) and helps clarify for the benefit of the rest of the community that which is being formalised.*

However, the authors have not evaluated whether their model is suitable for any of these endeavors, i.e., proof, communication (consumable by engineers), or clarification. They have evaluated that it is correct, per section 4.1. But they have not evaluated whether it is sufficiently engineered to support proof, per the concern in my review above, or whether its 9500 lines of commented Lem would be practically consumable by engineers and systems companies more than would be the some other representation (like the source code). 

Barring such empirical evaluations, we could attempt to judge the likelihood of their success based on the content of the paper itself. E.g., the third purpose of formalization — clarification — might be the key evaluation, and it would be reflected in the paper itself. But as I and most of the other reviewers have already said, it is hard to get much from the paper about the model, generalizable lessons that have been learned, etc. The model itself is explained in roughly 3.5/10 pages, in Section 5, much of which is so high-level as to not be particularly illuminating, at least not to me. And the authors’ response to reviewer requests for “essence” or other lessons learned seems to be that there aren’t any (?). I do concede, however, that all of the description of linker speak and unexpected oddness is showing the depth of the problem is indeed some contribution.

So in the end, I am still unpersuaded that the paper is sufficient on its own, but I will not stand in the way if others (strongly) disagree with me. 

Side note: I feel a little sad making this assessment because I know the authors have invested a lot of time to carry out ambitious work in an important area. I encourage the authors to find a way to evaluate these elements so that they can be communicated in the actual paper and readers will benefit from reading it. (Hopefully, doing so will not have to wait until the authors to go the full distance of building a verified compiler or some other substantial application that performs linking.)

====

Other comments/questions/nits:

P. 1: You say that a large fraction of code bases rely on linker
functionality; what is your basis for this claim? Clearly all programs
rely on linking, since all sizeable programs require separate
compilation. Programs that use libc rely on the wacky way that libc
has evolved with linking; but how many programs rely on non-obvious
linker features directly (e.g., require more than the "default
script")? I didn't see any argumentation about this; at best, "low
level" code like OSs might need it, but I wouldn't call this a large
fraction of all codebases.

P. 1: Likewise, you say linker speak is used haphazardly and often
incorrectly -- what's your basis for this claim?

P. 1: The code example with "alias" annotations is interesting
motivation, showing how linker speak bubbles into the main
language. But IIRC this example does not return, e.g., when explaining
your formal model; you instead focus on hello world. It would have
been nice to close the loop on this.

P. 2: I'm surprised that you have not cited Levine's "Linkers and
Loaders" book as a careful reference.

P. 5: "multiple defintions of ȯ files" --> "multiple definitions of .o
files"

P. 7: I found 4.1 hard to understand at first: Did you just use your
ELF model to parse a bunch of binaries? The first paragraph doesn't
say much; the next two describe the use of readelf and hexdump
equivalents whose output is diffed -- is this what you mean by
"testing?" 

P. 8: I didn't follow the paragraph at the end of 5.1 on symbolic
memory ranges. Footnote 6 was also confusing: -IX options might be
linked symbolically? What does it mean to link an option?

P. 9 "Our formalisation defines the linker script language's abstract
syntax in Lem." What does this mean? Do you mean that you hand
translated the script to what you believed to be an equivalent
representation in the Lem model? Or, do you have a parser, and that
this is what it produced, in Lem syntax, for the default script?

P. 11: There's some grammatical issue in the sentence 'This makes a
linker's in "programming the large" comparable ...'

===========================================================================
                            PLDI '16 Review #3E
                     Updated 18 Jan 2016 3:41:56pm EST
---------------------------------------------------------------------------
  Paper #3: The missing link: explaining ELF static linking, semantically
---------------------------------------------------------------------------

                 Reviewer expertise: X. Expert
                      Overall merit: B. OK paper, but I will not champion
                                        it

                         ===== Paper summary =====

This paper describes the complexities of formally modeling the linker
for the x86 variety of ABI. The work provides a specification in Lem
and in Isabelle/HOL.  The paper notes many real-world difficulties in
modeling linker-speak.

                      ===== Comments for author =====

Pros:

 + Linking is a critical aspect to consider for formal guarantees to
 be accurate, yet  ignored by most formal analysis. This paper helps
 change that.

 + The paper articulates a wide variety of implementation-dependent
 and/or obscure facts of linking.

 + The paper says the implementation produced by the specification is
 already being used by other projects, and has been used to check real
 executables.


Cons:

 - No concrete evaluation presented. The authors discuss checking the
   linker with hello_world and other programs, but I felt as though
   the "tool" aspect of the paper was shallow.

 - Linkers have many implementation-dependent components, and it's
   unclear whether making a formal specification for one is that
   revolutionary.

 - There were no insights in how to create a better formal linker
   standard.  This is my main comment: I would like to have had more
   lessons learned on what would be a "clean" specification.  The
   paper described many implementation-dependent and obscure features,
   but it wasn't clear that there could be a cleaner specification.


Commentary:
In a sense, this paper is trying to live in between two worlds.  On
the one hand, we have the implementation is the specification often
found in compilation (not just here, but also in parsing for example).
On the other, we have the PL-theory viewpoint that a formal
specification outside implementation is important.

The authors discuss implementation-specific behaviors, and at one
point, do say that differing implementations are at odds (S 5.1). This
reminded me that BFD is really, really buggy (at least the parts I
look at).  Did you end up formalizing bugs?  If it's implementation
specific behavior that you already have competing definitions for, why
is the formalization important?

I found the authors did a good job describing the intricacies of
linking.  I ended up walking away with the impression "wow, that
sounds complex", but I wasn't sure how widely applicable outside
formally documenting BFD the results would be.

I would love to see more text devoted to "what a clean linker would
look like".  I think quite a bit of the future work section could be
omitted. It's interesting to read, but I don't feel it adds scientific
weight to the paper.

Overall, I think this paper does a great job of bringing science to
some of the engineering/compiler internals process that adds to our
understanding. While I would have liked to see them go beyond the
single formal model and explore other areas (e.g., are there places in
the linking that are tricky for no good reason which the formalism
helps us reason about?), I think there is obvious limits to what can
be in one paper.

I hope the source is indeed released so others can benefit.  My score
is probably a 0.5 on the anticipation it will be made available.

Small things:

 - I'm not convinced your characterization of ELF is the standard is
   true.  I think focusing on ELF is great, and it is a very widely
   used standard and inspiration (along with it's predecessors) for a
   great many formats.  However, I felt there was not enough attention
   paid to different OS linker differences, esp. ELF vs. Microsoft
   with PE.

 - I wrote "Meh" next to bullet point one for contributions.  I find
   this sort of contribution underwhelming. It's great for a blog; why
   for a scientific paper that should advance the field? I'm guessing
   there is a community of compiler developers who know many if not
   all these things.  I would leave this off the contribution list as
   it's a contribution of the discussion, but not really a
   contribution to science.

 - 5.1 contrasts deterministic with non-deterministic.  However, I
   think you mean implementation-specific (the word looseness also
   used is good).  I would take non-deterministic to mean there is a
   coin flip somewhere, which doesn't seem the case here.  In other
   words, I would prefer not using the word non-deterministic; it
   seems to be the wrong word.

 - S5.2 first says 891 objects, then 897 objects. I was confused.

 - Intro: "Previous semantic work on linking..." you should have a
   citation to which previous work you are referring.

 - S3.2: "libary" -> "library"

 - You appear to have de-anonymized with references.

== ADDED AFTER AUTHOR RESPONSE ==

Thank you for your response.  It has no changed the substance of my review. I am very positive, but do see a problem where it's unclear what the formalization will ultimately yield beyond a precise description of the studied linker.  PLEASE CONTINUE THIS WORK!  I really believe that the model will end up being useful, and my intuition is creating the model is the hard part and you can focus now on formalizing some theorems/insights.

===========================================================================
           Response by Peter Sewell <Peter.Sewell@cl.cam.ac.uk>
---------------------------------------------------------------------------
We thank the reviewers for their helpful comments.  We first note that the
reviewers collectively ask for three things:

  1. Using the model to prove substantial theorems about the linking
  process (A, D); 
  2. Using "lessons learnt" from the specification process to
  investigate how linking could be made less ad-hoc (B, D, E); and
  3. Extending existing verified compilers such as CompCert to handle
  linking (D).

Each of these would be another (very interesting, multi-person-year)
project in its own right, remembering that any mechanised proof is a
major undertaking, and trying to combine any of them with the
current work would take it beyond what can reasonably be explained in
a single PLDI paper.  The fact that one can start seriously
contemplating such work is itself a contribution of our current paper,
and this is exactly what we meant when we wrote of "bringing linking
into the formal discourse".

To address some other important points:

  * For A: all recursive functions, both in the ELF and linker
    formalisation, have been proved terminating.  This already
    establishes the basic usability of the model for proof.

  * For A and D: while "hello world" is close to the simplest C
    program, it is far from a simple link job, since it necessarily
    includes a whole C library.  We focus on it because linking it
    exercises most features in Section 3; our tool does scale to
    larger programs, e.g. bzip2.

  * Reviewers A and D suggest that the sole point of a formalisation
    is to support proof, and question the relationship between a model
    and an implementation in a restrictive language.  What we have is
    a "model" in several important ways: (a) it is mathematically
    precise, (b) it is written with clarity as the prime goal, and (c)
    it explicitly addresses the various forms of loose specification
    involved here, rather than fixing on some implementation choice.
    Some of this would be more apparent if space had permitted more
    detail of the formalisation in the main body of the paper.
    Supporting proof is only one possible goal among many of a
    formalisation: we are equally concerned with providing a clear
    specification, supporting discussion at a level of abstraction
    above C implementations, and providing a test oracle.

  * Reviewers B and D expected to see the "essence" of linking.  While
    (we agree) it would be great if such a thing did exist, a clear
    conclusion from our work is that it does not; instead, there are
    many important use-cases for different aspects of linker-speak,
    which are supported by many more-or-less ad hoc mechanisms.  The
    behaviour of current linkers is essentially a contract between
    hundreds of disparate compilers and other tools that now rely on
    this behaviour in order to work correctly, so it cannot simply be
    "fixed" in isolation.  Instead, we have to understand all those
    use-cases and mechanisms in depth before going forwards, if one
    wants to pursue (2) above, as E suggests.  Our work provides a
    concise and precise characterisation of many of the issues that
    will have to be addressed.  Previous formal work on linking, in
    contrast, has used highly idealised models.

  * Reviewers B, D and E suggest that what we write (eg in Section 3)
    is present in public documentation or known to compiler
    developers.  Some of it is, of course, but we have have anecdotal
    data (omitted from the submission for anonymity) showing that many
    compiler developers have a very weak understanding of linking,
    (e.g. in OCaml, GHC, and Graal contexts). The existing literature
    (e.g Levine) explains how a linker works, not why and how
    programmers use it.

  * For E: our formal model is included as supplementary material, and
    we intend to make it available under an open-source licence before
    publication.



Specific replies to targetted questions and comments now follow.

> ===========================================================================
>                             PLDI '16 Review #3A
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: C. Weak paper, though I will not fight
>                                         strongly against it

> - The model linker seems to yet scale only to small programs and a C
> library that, while large, is not the standard Linux one

This is true, although (as we noted above), our use of a "hello, world"
example does not mean we can't process larger link jobs. As of the
submission date, our spec could execute a successful link of bzip2 and
similar-sized programs (bzip2 is 8000 raw lines of C, not including the
C library it links with). The apparent obstacles to larger link jobs are
very minor (OCaml stack overflows from non-tail-recursive library
functions, and generally unoptimised code). Our use of uClibc is purely
to avoid a small number of GNU extensions that are not yet in our
specification, notably IRELATIVE relocations. 

> - No interesting theorems have yet been proved

Whilst to some extent this is true (modulo our comment below about
proving termination of functions used to define the linker itself) we
believe that proving anything interesting about the model is at
*least* another paper's worth of work, and there's a limit to what we
can sensibly report (and do) within the confines of a single paper.

> Modeling a linker without proving anything about it is basically just
> reimplementing a linker using a restrictive programming language.  The
> level of detail the paper gives about how it does this makes it sound
> well-structured and suitable as a basis for future work, but a real
> demonstration of its value would be by constructing proofs. The
> theorem about relocation correctness that the paper describes
> formulating would sounds like a good demonstration, but if the
> author(s) haven't yet proved it, we don't have much confidence that
> its statement is correct. 

As we discussed at the top, the model can provide value in several ways, not
only proof, all of which we fully intend to support.

> are the 1500 lines of handwritten termination proofs are
> roughly proving the termination of ELF file parsing as in "readelf",
> or the termination of linking as in "ld"? The latter would be a
> somewhat more interesting result.

They are doing both, and we will rewrite this text to make this clear.

We should mention that in Linker_script.thy submitted with this paper
one large recursive function is commented out in the Isabelle/HOL
extraction, along with several other non-recursive functions that depend
upon this. This definition provokes a bug in Isabelle, reported to the
Isabelle mailing list in late 2015. We are still waiting on a fix. After
submission, this function was rewritten by hand to work around the bug,
and all remaining termination proofs completed.

> The case for the
> practical importance of the paper's work would be stronger, though, if
> the paper could give a more pernicious example of linking causing
> undesirable behavior, such as something silently incorrect, or clearly
> contrary to programmer intent. Perhaps this position could be filled
> by expanding on the claim in the introduction about Figure 1 that the
> calls to _int_malloc might not go the function defined in that figure?
> It wasn't clear to me which linker behavior could be the cause of
> that.

Section 3.2 refers back to the opening malloc-based example throughout,
including in the "Build-time substitution", "Load-time substitution",
"Aliases" and "Topology alternatives" paragraphs. The first two of these
explain mechanisms by which the given "_int_malloc" might be bypassed,
while the latter two explain the purpose of the alias directives shown.

> I would have been curious to hear a bit more about the further bugs in
> the ELF specification that were revealed not by testing on 7k binaries
> but by the Isabelle/HOL proof process for termination and "various
> other lemmas".

Isabelle proofs and validation were carried out concurrently.  As a
result, several very basic errors were in fact highlighted not by
testing, but by preliminary work in Isabelle where we experimented with
proof using the model. These included an incorrect byte ordering in the
parsing function for 8-byte types like elf64_xword that caused
roundtripping to fail.  This would eventually have been identified by
validation, but was in fact identified within Isabelle itself.

Some bugs that would not have been identified by validation include
certain functions that did not terminate on all inputs, in the linker
and in the ELF model, for example parsing functions relating to symbol
versioning were originally not provably terminating on all inputs due
to a silly error and this was only brought to light when we attempted
to prove them so.  Bugs of this sort would only have been uncovered by
validation if extremely lucky (and having a failing termination proof
makes tracking down the cause of non- termination much easier than
just observing a non-terminating program).

Another recent bug in the specification uncovered by working with the
model in Isabelle was a lack of padding in NOTE sections to align
certain fields up to a 64- or 32-bit boundary.  This was not uncovered
during validation as the actual contents of NOTE sections were never
systematically decoded during validation (readelf does not do this for
--program-headers, --section-headers, etc.).

In short: Isabelle extraction was used relatively early and new
versions of the model have continuously been extracted to Isabelle,
proofs and definitions updated, and so on, and has played a role in
validation of even basic definitions itself, as has trying to use our
ELF model with other tools as a library.

> "normally archives can only refer to objects appearing to their left":
> did you mean "appearing to their right"? 

Yes, good catch... this should say "archives can only be referenced by objects
appearing to their left". (Note that archives themselves can potentially
reference to the left *or* right.)

> ===========================================================================
>                             PLDI '16 Review #3B
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: Y. Knowledgeable
>                       Overall merit: B. OK paper, but I will not champion
>                                         it

> The main thing missing from this paper is insights. The descriptive
> aspects can already be found in public documentation (although it is
> nice to have them collated and summarized here).  What did we learn
> from this whole exercise? We already knew that linking is messy. The
> high-level vision is somewhat missing from this paper.

As we discussed at the top, our finding (or insight) is that there is no
simple essence, since linker-speak addresses real but disparate programmer
requirements. Designing a less ad-hoc alternative could certainly be done,
and would build on our work.

> The paper refers to an ELF "model" and "formalization", but such a
> formalization is neither contained in the paper nor (as far as I could
> tell) included as a reference. The "model" in this paper appears to be
> just a description of the HOL implementation's phases, which mirrors a
> real linker's phases.

By "model" we mean the Lem specification of ELF and the linker, which
we included as additional material in our submission, along with the
extraction of an Isabelle/HOL version of this model.  See the
discussion of model vs implementation in the main response.

> The evaluation (Section 4.1) discusses validating the model against 7k
> binaries. What sources of incompleteness in source specification
> documents were found? Did you update the model to deal with all corner
> cases exposed by these binaries? A table summarizing the results from
> this large validation would make it more clear what exactly
> happened. Also, what bugs did you discover when translating the model
> to Isabelle/HOL?

Most ELF binaries found "in the wild" contain section types, symbol
types, relocation types, and so on that are not mentioned explicitly in
any formal or semi-formal document.  Examples include many GNU-specific
extensions, especially those related to prelinking. These are not
described in the Linux Standard Base document (LSB) which does partially
describe most GNU extensions (GNU HASH, etc).

The ELF model was updated to accommodate these features as they were
found during validation.  we therefore do not claim that our model is
comprehensive --- validating against e.g. MIPS binaries, or even PowerPC
binaries produced by an obscure compiler, may reveal yet more sources of
incompleteness that will need to be added to the formal model on
as-needed basis.

There were other cases where the ELF specification is unclear or
ambiguous. One example was a series of PowerPC64 binaries that contained
a zero-sized segment whose offset was beyond the end of the file, 
causing an exception when validating -- a case not explicitly allowed
in the specification documents.

> On a related note, this paper is very grounded in current
> implementation details of linkers -- but that implementation may
> change. How does that affect the results? Why formalize all the warts
> in the linker instead of fixing some of them?

As noted at the top, linker behaviour is a contract shared between many
compilers and linkers, all of which would need updating; working with
existing warts avoids this.

> ===========================================================================
>                             PLDI '16 Review #3C
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: B. OK paper, but I will not champion
>                                         it

> The linking model is more difficult to validate: linker behavior
> contains non-deterministic computations and cannot be fully captured
> by the proposed model. This means that a standard C library cannot be
> currently linked using the model. 

To clarify: the spec's incorporation of non-determinism (a.k.a.
looseness), which is intentional, is unrelated to the current lack of
glibc support.

> This is a well-written paper that describes the experience in
> formalizing ELF and its linking for the Linux platform. The actual
> model used is not really seen in the text, which is more concerned
> with the many obstacles that were encountered during formalization,
> because of the current state of ELF linking semantics and
> implementation.

This was a conscious choice.  This type of modelling of real-world
systems does not lead to neat, easily digestible definitions that fit
nicely into a paper of this form, like e.g. typing rules of small
calculi or toy programming languages.  As a result, we thought it better
not to fill the paper with definitions and instead concentrate our
effort on highlighting *why* the definitions are like they are.

> 1. How difficult are the termination proofs? The whole effort seems
> big enough for any attempt at proving things via a proof assistant
> being painful. Any remarks on that?

The termination proofs in Isabelle/HOL were not especially onerous
(modulo working around bugs in Isabelle itself, as mentioned above)
except for their size, which is essentially a reflection of the (quite
large) size of the formalisation itself.

The reviewer is correct in saying that proving anything non-trivial
using this formalisation will be a significant amount of work, hence
our comments at the start of the rebuttal, though given recent large
formalisation projects (e.g.  CompCert, seL4, CerCo, and so on) this
is to be expected for any new (or interesting) contemporary
formalisation project.

> 2. What kinds of bugs did the extraction/proving in Isabelle/HOL find?
> Why weren't they captured by testing? Were they different in nature?

We have answered an identical question above.

> 3. Why are the proofs also done in Coq/HOL4? Isn't one proof assistant
> enough? Is this related to integration with CakeML or other formalized
> compilers?

Unfortunately, the community has not rallied around any single theorem
prover.  We have CompCert written in Coq, CakeML written in HOL4, seL4
written in Isabelle/HOL, and so on, and we envisage that our work will
be at least interesting to people involved in all of these projects. To
maximise the utility of our model, we decided to aim to extract our
definitions to all backends that Lem supports. 

The Isabelle/HOL extraction is the only extraction that currently
includes both definitions and termination proofs for recursive functions
(hence the generation of simplification rules associated with those
functions).  The HOL4 extraction consists of definitions only. We
unearthed a fairly serious bug in the termination mechanism of HOL4 that
is yet to be fixed but prevents us from completing proofs of termination
for recursive, monadic functions. (A tentative fix by the HOL4 team was
pushed but then reverted).  A Coq extraction is only planned at this
stage, and is not yet concrete.

> 4. (Possibly related to the previous remark.) How easy is it for a
> "verified" linker like the one proposed to be interfaced with a
> verified C/C++ compiler? In other words, how easy is it for the
> linking specification (that the linker follows) to be combined with a
> C/C++ spec (that e.g. CompCert follows)? Does this require using the
> same (e.g. Coq) language/infrastructure?

This is another paper's worth of work (at least!)  Note that we have
explicitly listed this - the extension of existing verified compilers
like CompCert and CakeML - as possible further work and is certainly
something that we are interested in.

> 5. Although the text is good, the reader is left with few technical
> insights as to how to write such a model (apart from reading the
> attached code). It feels strange that such a seemingly big and complex
> piece of work does not demonstrate its technical "guts". More
> technical stuff (like figures 2 and 3, the "symbolic memory images" of
> section 5.1 or the "interpreter" and "eligibility predicate" of
> section 5.2) would be welcome, to give a flavor of the way that such a
> model can be formulated. Were there any expressive power problems
> writing the ELF specification? Did the authors need any clever hacks
> or principled programming to manage the specs of ELF and linking?

We certainly ran into difficulties with the Lem language; our
specification was the biggest Lem codebase yet, by some distance.
Abstracting over 32- and 64-bit ELF details is not possible in Lem, so
much code is duplicated and we effectively use the 64-bit definitions as
our "internal representation" in many places. Limitations of Lem
typeclasses and lack of support for cyclic references also required
hackarounds. We could easily add a brief discussion of these, and/or 
more snippets of the "guts" of the spec (although space is tight).

> 6. The paper is not about modules at the level of the source at all;
> it is about object code and its linking in a specific setting: ELF
> linking on a Linux/Android platform. Since it is language-agnostic, it
> seems that some of the higher-level related work should be seen as
> complementary to it. Is this the case? Can this work be combined with
> any of the related work?

Certainly, formally specifying how source language implementations map
down to linker-speak is worthwhile, as we noted in section 7. Doing so
would be a complementary but separate (and large) piece of work.

> ===========================================================================
>                             PLDI '16 Review #3D
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: D. Reject

> First, the paper says very little about the substance of the model. It
> presents a long list of surprising and disturbing features that have
> crept into modern linkers. It presents some meta information about how
> its ELF model was evaluated (running it on binaries) but not much
> about the model itself. It describes, prosaically for a given example,
> the stages of static linking, as implemented by the model (and actual
> linkers, presumably). And it points out that the model, extracted, can
> perform real work. But I expected the authors to teach me something
> interesting, e.g., the "essence" of linking, perhaps, in a way that is
> possibly more realistic or relevant than prior idealized
> presentations. At the end of the paper, I had basically learned that
> linking is an arcane activity perhaps in need of a re-think. I did not
> have any larger insight than I did when I started.

As mentioned at the start of this rebuttal one of the main lessons one
can draw from this work is that there is no simple "essence" of
linking, and indeed previous (theoretical) works that have worked on
this basis only consider a small number of the tasks that linkers are
actually used for in practice.

Further, one very interesting line of future work is in interfacing
our specification with existing verified compilers like CompCert and
CakeML.  It would be highly advantageous if code produced by these
compilers could work alongside and interact with "real" existing code
produced by other untrusted tools, hence the complete modelling of
linker features is necessary.  As a result, we chose to model
everything, arcana and all, as-is.

One reason for our enumerating the features of linker-speak is to show
that linker complexity is grounded in real programmer requirements,
*not* feature creep.

> Second, there is no substantial use of the artifact that is reported
> in any detail, and hence the model's true purpose is not really
> evaluated. 

> Instead, the whole point of a formal model is proving things
> with it. To show that the model truly facilitates proof, we have to
> prove something. Poor design decisions might get in the way of
> effective proofs. But no serious applicatiosn of the model have been
> carried out; only some termination proofs for recursive linking that
> the paper barely reports on.

We see formal proof as only one point of formalisation.  Another is
the ability to precisely communicate with external engineers and
systems companies, in a precise manner about complex mechanisms and
artefacts at a level of abstraction above a C implementation.  We also
see formalisation as a process that enables lots of other research
(not necessarily formal proof) and helps clarify for the benefit of
the rest of the community that which is being formalised.

Further, proving anything remotely interesting about a model of this
scale is another substantial paper's worth of work, at least.  We of
course plan in the near future to start proving things using our
model, and mention many ideas along these lines in our "future work"
section, but there are limits in what can be done effectively within
the confines of a single PLDI paper.

> In short, this is impressive work that will surely be useful. But I
> don't see much value in this particular paper about it, just yet.
> 
> Other comments/questions/nits:
> 
> P. 1: You say that a large fraction of code bases rely on linker
> functionality; what is your basis for this claim? Clearly all programs
> rely on linking, since all sizeable programs require separate
> compilation. Programs that use libc rely on the wacky way that libc
> has evolved with linking; but how many programs rely on non-obvious
> linker features directly (e.g., require more than the "default
> script")?

It is true that few codebases use non-default linker scripts. However,
the other linker features mentioned in section 3 are much more
commonly used. The "large fraction" of codebases certainly includes
most large libraries, which have to grapple with symbol visibility
(viz. Drepper's guide for library authors) and often also with
versioning. Besides libraries, it is not uncommon for large
application codebases to ship with wrapper scripts that play tricks
with LD_PRELOAD or LD_LIBRARY_PATH (e.g. Adobe Reader or Google Chrome
both do this). Furthermore, any C++ codebase is dependent on link-time
uniqueness handling. Threaded libraries fairly often make use of weak
references (to test for the "single-threaded application" case). Code
featuring plug-in systems uses the dynamic linking API (dlopen()
etc.). And so on.  Although no one feature is common, almost no large
library, and few large applications, gets away without using any of
these features.

> P. 1: Likewise, you say linker speak is used haphazardly and often
> incorrectly -- what's your basis for this claim?

"Haphazardly" refers to the *specification* of linker-speak, not its
usage. We do say that linker-speak is often used "incorrectly or... in
unportable or fragile ways". Section 3 already contains several
anecdotes to this effect: we note how substitution via LD_PRELOAD is
inherently unsound; how link-time interposition (with --wrap) fails to
capture all references; and how an often-recommended option
(-fvisibility=hidden) breaks the source semantics of C++. A further
example is that clients of dlsym() are generally not robust to symbol
versioning.

> P. 1: The code example with "alias" annotations is interesting
> motivation, showing how linker speak bubbles into the main
> language. But IIRC this example does not return, e.g., when explaining
> your formal model; you instead focus on hello world. It would have
> been nice to close the loop on this.

Linking a hello-world program involves *all* the C library code that
we show in Section 3 (or close equivalents). For example, many linker
aliases (coming from libc) are processed when linking hello-world. We
choose hello-world precisely because although it is a small/simple
program, linking it is *not at all* a simple link job.

> P. 2: I'm surprised that you have not cited Levine's "Linkers and
> Loaders" book as a careful reference.

This was an oversight, though it's worth noting the book focusses on
core linking mechanisms rather than how they are used.

> P. 7: I found 4.1 hard to understand at first: Did you just use your
> ELF model to parse a bunch of binaries? The first paragraph doesn't
> say much; the next two describe the use of readelf and hexdump
> equivalents whose output is diffed -- is this what you mean by
> "testing?"

We parsed binaries using an OCaml version of our Lem model, extracted
by Lem itself, and wrote a tool around this code to mimic the output
of readelf when passed flags like --section-headers, --file-header,
--relocs, and so on, using information extracted from the parsed
binaries by the Lem model.  We then mimicked the formatting and output
of readelf's output and compared the output on ~7000 binaries
automatically using a script and a diff tool.

We also separately used hexdump to compare the byte-for-byte output of
our code when a binary was read in and then blitted back out, again
using diff, to ensure that our parsing and blitting code was correct.

> P. 8: I didn't follow the paragraph at the end of 5.1 on symbolic
> memory ranges. Footnote 6 was also confusing: -IX options might be
> linked symbolically? What does it mean to link an option?

The point here is that "options" of the form "-lX" actually denote
libraries (e.g. -lc is mapped to /usr/lib/libc.a or similar). It is
unfortunate that the typeface makes 'l' appear like 'I'.

> P. 9 "Our formalisation defines the linker script language's abstract
> syntax in Lem." What does this mean? Do you mean that you hand
> translated the script to what you believed to be an equivalent
> representation in the Lem model? Or, do you have a parser, and that
> this is what it produced, in Lem syntax, for the default script?

The former: for now, we hand-translate the script into a Lem value.


> ===========================================================================
>                             PLDI '16 Review #3E
> ---------------------------------------------------------------------------
>   Paper #3: The missing link: explaining ELF static linking, semantically
> ---------------------------------------------------------------------------
> 
>                  Reviewer expertise: X. Expert
>                       Overall merit: A. Good paper, I will champion it

>  - There were no insights in how to create a better formal linker
>    standard.  This is my main comment: I would like to have had more
>    lessons learned on what would be a "clean" specification.  The
>    paper described many implementation-dependent and obscure features,
>    but it wasn't clear that there could be a cleaner specification.

Please see our answer at the top.

> The authors discuss implementation-specific behaviors, and at one
> point, do say that differing implementations are at odds (S 5.1). This
> reminded me that BFD is really, really buggy (at least the parts I
> look at).  Did you end up formalizing bugs?  If it's implementation
> specific behavior that you already have competing definitions for, why
> is the formalization important?

It is worth reiterating that we're not formalising BFD per se, nor
solely testing against it (as noted in 5.2 we already use the gold
linker, which doesn't use BFD, and will be able to use the new llvm
linker once it's more mature). ELF linking is more general than BFD
behavior, and we aspire to capture all legal behaviors (as our
discussion of looseness noted, \S 5.3).  Of course, once a linker is
widely-used, like the BFD-based GNU linker, other linkers aim to be
"bug-compatible", making the bug de facto "standard" behavior.

> I hope the source is indeed released so others can benefit.  My score
> is probably a 0.5 on the anticipation it will be made available.

Certainly, as we noted at the top, it will be.

> Small things:
> 
>  - I'm not convinced your characterization of ELF is the standard is
>    true.  I think focusing on ELF is great, and it is a very widely
>    used standard and inspiration (along with it's predecessors) for a
>    great many formats.  However, I felt there was not enough attention
>    paid to different OS linker differences, esp. ELF vs. Microsoft
>    with PE.

We are careful not to proclaim ELF as *the* standard in Section 2.
Rather, we claim that ELF is the de facto standard executable and
linkable format on GNU/Linux and BSD and related operating systems
(which it is).  Further, we acknowledge that Windows and MacOS each
have their own executable and linkable formats in PE and Mach-O that
whilst incompatible either have broadly similar features to ELF.

Certainly, we can envisage that similar work specifying PE, a.out,
Mach-O, etc.  linking is possible in analogy with this work.

>  - I wrote "Meh" next to bullet point one for contributions.  I find
>    this sort of contribution underwhelming. It's great for a blog; why
>    for a scientific paper that should advance the field? I'm guessing
>    there is a community of compiler developers who know many if not
>    all these things.  I would leave this off the contribution list as
>    it's a contribution of the discussion, but not really a
>    contribution to science.

Please see above.

===========================================================================
                                  Comment
---------------------------------------------------------------------------
Summary of PC discussion:

The discussion focused on whether the significant effort and realism underlying the artifact was sufficient to overcome its lack of evaluation as a formal model. Ultimately, the reviewers felt that more demonstration of utility (and/or more justification/details about the model, in the paper) is needed for the paper to be acceptable.

